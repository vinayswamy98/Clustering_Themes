import os
import time
import pandas as pd
from PyPDF2 import PdfReader
from langchain_community.embeddings import SentenceTransformerEmbeddings
from langchain_community.vectorstores import Chroma
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_anthropic import ChatAnthropic
from langchain_core.prompts import PromptTemplate
from langchain_core.pydantic_v1 import BaseModel, Field, validator
from langchain_core.output_parsers import PydanticOutputParser
from tqdm import tqdm
import argparse
from typing import Dict, List

# Configuration
PARAMETERS = []
EMBEDDING_MODEL = 'all-MiniLM-L6-v2'
CHROMA_DB_PATH = './chroma_langchain'
BATCH_SIZE = 10
RATE_LIMIT_DELAY = 1
TOP_K = 5

# LangChain setup
embeddings = SentenceTransformerEmbeddings(model_name=EMBEDDING_MODEL)
vectorstore = Chroma(persist_directory=CHROMA_DB_PATH, embedding_function=embeddings)
llm = ChatAnthropic(model="claude-3-haiku-20240307", api_key=os.environ['ANTHROPIC_API_KEY'])

# Pydantic models
class reasonFeedback(BaseModel):
    reason_aligned_with_guidelines: bool = Field(..., description="True if reason follows guideline structure")
    missing_guideline_elements: List[str] = Field(default_factory=list, description="List of missing required elements")
    description_relevant_points_used: bool = Field(..., description="True if key description points are used")
    reason_quality: str = Field(..., description="STRONG, ADEQUATE, or WEAK")
    improved_reason: str = Field(..., description="Full improved reason text")
    confidence: float = Field(..., ge=0.0, le=1.0, description="Confidence score 0.0-1.0")

    @validator('reason_quality')
    def valid_quality(cls, v):
        if v not in ["STRONG", "ADEQUATE", "WEAK"]:
            raise ValueError("Must be STRONG, ADEQUATE, or WEAK")
        return v

class ValidationResult(BaseModel):
    __root__: Dict[str, reasonFeedback] = Field(..., description="Mapping of parameter to feedback")

def extract_pdf_text(pdf_path):
    reader = PdfReader(pdf_path)
    return "\n".join(page.extract_text() or "" for page in reader.pages)

def process_guidelines(pdf_path, excel1_path, excel2_path):
    docs = []

    # PDF
    pdf_text = extract_pdf_text(pdf_path)
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_text(pdf_text)
    docs.extend([{"page_content": c, "metadata": {"type": "pdf", "parameter": "general"}} for c in chunks])

    # Excel 1
    df1 = pd.read_excel(excel1_path)
    for _, row in df1.iterrows():
        param = row['Parameter']
        for r in range(1, 6):
            content = str(row[f'score {r}']).strip()
            if content:
                docs.append({
                    "page_content": content,
                    "metadata": {"type": "score_scale", "parameter": param, "score": r}
                })

    # Excel 2
    df2 = pd.read_excel(excel2_path)
    for _, row in df2.iterrows():
        param = row['Parameter']
        good = str(row['Good reason Example']).strip()
        weak = str(row['Weak reason Example']).strip()
        if good:
            docs.append({"page_content": good, "metadata": {"type": "good_reason", "parameter": param}})
        if weak:
            docs.append({"page_content": weak, "metadata": {"type": "weak_reason", "parameter": param}})

    return docs

def build_vectorstore(pdf_path, excel1_path, excel2_path):
    if os.path.exists(CHROMA_DB_PATH) and os.listdir(CHROMA_DB_PATH):
        print("Loaded existing vectorstore")
        return

    docs = process_guidelines(pdf_path, excel1_path, excel2_path)
    texts = [d["page_content"] for d in docs]
    metadatas = [d["metadata"] for d in docs]
    vectorstore.add_texts(texts, metadatas=metadatas)
    vectorstore.persist()
    print("Built and persisted vectorstore")

def validate_record(title, description, param_scores, param_reasons):
    retriever = vectorstore.as_retriever(search_kwargs={"k": TOP_K * 3})
    retrieved = retriever.invoke("score scale definitions and good/weak reason examples")
    context = "\n".join([
        f"[{d.metadata.get('parameter','general')}] {d.page_content}"
        for d in retrieved
    ])

    param_inputs = []
    for p in PARAMETERS:
        param_inputs.append(f"""
        PARAMETER: {p}
        score: {param_scores[p]}
        reason: {param_reasons[p]}
        """)

    parser = PydanticOutputParser(pydantic_object=ValidationResult)

    prompt = f"""
    You are validating **reasons against guidelines** using description for context.

    Title: {title}
    Description (context only): {description}

    Guidelines:
    {context}

    For each parameter:
    - Check alignment with good reason patterns
    - Identify missing guideline elements
    - Confirm use of key description points
    - Suggest improved reason

    Parameters:
    {''.join(param_inputs)}

    {parser.get_format_instructions()}
    """

    chain = PromptTemplate.from_template(prompt) | llm | parser
    try:
        result = chain.invoke({})
        time.sleep(RATE_LIMIT_DELAY)
        return result.__root__
    except Exception as e:
        print(f"Validation error: {e}")
        fallback = {}
        for p in PARAMETERS:
            fallback[p] = reasonFeedback(
                reason_aligned_with_guidelines=False,
                missing_guideline_elements=["API error"],
                description_relevant_points_used=False,
                reason_quality="WEAK",
                improved_reason="",
                confidence=0.0
            )
        return fallback

def main(pdf_path, excel1_path, excel2_path, excel3_path, output_path):
    build_vectorstore(pdf_path, excel1_path, excel2_path)

    df_records = pd.read_excel(excel3_path).head(500)
    columns = ['Title', 'Description']
    for p in PARAMETERS:
        columns += [
            f'{p}_score', f'{p}_reason',
            f'{p}_reason_Aligned', f'{p}_Missing_Elements',
            f'{p}_Description_Used', f'{p}_reason_Quality',
            f'{p}_Improved_reason', f'{p}_Confidence'
        ]
    results_df = pd.DataFrame(columns=columns)

    for i in tqdm(range(0, len(df_records), BATCH_SIZE), desc="Validating records"):
        batch = df_records.iloc[i:i+BATCH_SIZE]
        for _, row in batch.iterrows():
            title, desc = row['Title'], row['Description']
            scores = {p: row[f'{p}_score'] for p in PARAMETERS}
            reasons = {p: row[f'{p}_reason'] for p in PARAMETERS}

            vals = validate_record(title, desc, scores, reasons)

            new_row = {"Title": title, "Description": desc}
            for p in PARAMETERS:
                r = vals.get(p, {})
                new_row.update({
                    f'{p}_score': scores[p],
                    f'{p}_reason': reasons[p],
                    f'{p}_reason_Aligned': getattr(r, 'reason_aligned_with_guidelines', False),
                    f'{p}_Missing_Elements': ', '.join(getattr(r, 'missing_guideline_elements', [])),
                    f'{p}_Description_Used': getattr(r, 'description_relevant_points_used', False),
                    f'{p}_reason_Quality': getattr(r, 'reason_quality', 'WEAK'),
                    f'{p}_Improved_reason': getattr(r, 'improved_reason', ''),
                    f'{p}_Confidence': getattr(r, 'confidence', 0.0)
                })
            results_df = pd.concat([results_df, pd.DataFrame([new_row])], ignore_index=True)

    # Summary
    summary = {}
    for p in PARAMETERS:
        aligned = results_df[f'{p}_reason_Aligned'].sum()
        strong = (results_df[f'{p}_reason_Quality'] == 'STRONG').sum()
        total = len(results_df)
        summary[p] = {
            'Aligned': aligned,
            'Strong': strong,
            'Aligned Rate': aligned / total,
            'Strong Rate': strong / total
        }
    overall_aligned = sum(s['Aligned'] for s in summary.values()) / (len(PARAMETERS) * total)
    summary['Overall'] = {'Aligned Rate': overall_aligned}
    summary_df = pd.DataFrame(summary).T

    # Export
    with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
        results_df.to_excel(writer, sheet_name='Validation Results', index=False)
        summary_df.to_excel(writer, sheet_name='Summary')
    print(f"Validation complete. Report saved to: {output_path}")

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="reason Validation System")
    parser.add_argument('--pdf', required=True, help='Path to guidelines PDF')
    parser.add_argument('--excel1', required=True, help='Path to score scale Excel')
    parser.add_argument('--excel2', required=True, help='Path to reason examples Excel')
    parser.add_argument('--excel3', required=True, help='Path to 500 records Excel')
    parser.add_argument('--output', default='reason_validation_report.xlsx', help='Output Excel path')
    args = parser.parse_args()

    main(args.pdf, args.excel1, args.excel2, args.excel3, args.output)
